{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19862d90",
   "metadata": {},
   "source": [
    "Testing experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9af6c18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulate_module import *\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "np.random.seed(1234)\n",
    "import torch\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6bcd74b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.        ],\n",
       "       [-0.89473684],\n",
       "       [-0.78947368],\n",
       "       [-0.68421053],\n",
       "       [-0.57894737],\n",
       "       [-0.47368421],\n",
       "       [-0.36842105],\n",
       "       [-0.26315789],\n",
       "       [-0.15789474],\n",
       "       [-0.05263158],\n",
       "       [ 0.05263158],\n",
       "       [ 0.15789474],\n",
       "       [ 0.26315789],\n",
       "       [ 0.36842105],\n",
       "       [ 0.47368421],\n",
       "       [ 0.57894737],\n",
       "       [ 0.68421053],\n",
       "       [ 0.78947368],\n",
       "       [ 0.89473684],\n",
       "       [ 1.        ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(-1, 1, 20)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d27a142",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_setting = {\"high_bw\": [10, .2],\n",
    "                \"medium_bw\": [1, .2],\n",
    "                \"low_bw\": [.5, .2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fb08e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"n_tasks\": 15,\n",
    "    \"conservative\": True,\n",
    "    \"target_test_size\": 0.8,\n",
    "    \"model_type\": \"nn\",\n",
    "    \"base_output_dir\": \"test\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae26cadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args[\"model_type\"] == \"lm\":\n",
    "    model_class = lm()\n",
    "    loss_fn = mse\n",
    "elif args[\"model_type\"] == \"nn\":\n",
    "    model_class = nn()\n",
    "    loss_fn =  torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "199697f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"high_bw\"\n",
    "    # set directory\n",
    "if args[\"conservative\"]:\n",
    "    data_path = Path(args[\"base_output_dir\"]) / Path(\"model_\" + args[\"model_type\"] + \"/conservative_derived_data\")\n",
    "else:\n",
    "    data_path = Path(args[\"base_output_dir\"]) / Path(\"model_\" + args[\"model_type\"] + \"/derived_data\")\n",
    "data_path = Path(data_path)\n",
    "working_path = data_path / s\n",
    "working_path.mkdir(parents = True, exist_ok = True)\n",
    "# generate data ------------------------------------------------\n",
    "np.random.seed(1234)\n",
    "f, betas, zs = random_functions(args[\"n_tasks\"], 6,\n",
    "                                sigma_between = sigma_setting[s][0],\n",
    "                                sigma_within = sigma_setting[s][-1])\n",
    "result = []\n",
    "for i, fi in enumerate(f):\n",
    "    x = np.random.uniform(0, 1, 100)\n",
    "    result.append({\n",
    "        \"task\": i,\n",
    "        \"x\": x,\n",
    "        \"f\": fi(x)\n",
    "        \n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccd51369",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k , _ in enumerate(result):\n",
    "    \n",
    "    result[k][\"p\"] = 1 / (1 + np.exp(- result[k][\"x\"])) + np.random.normal(0, .1, len(result[k][\"x\"]))\n",
    "    result[k][\"y\"] = np.zeros((len(result[k][\"x\"]), 2))\n",
    "    for i, v in enumerate(result[k][\"p\"]):\n",
    "        if v > np.mean(result[k][\"p\"]):\n",
    "            result[k][\"y\"][i, 1] = 1\n",
    "        else:\n",
    "            result[k][\"y\"][i, 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c03d6eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k , _ in enumerate(result):\n",
    "    \n",
    "    result[k][\"p\"] = 1 / (1 + np.exp(- result[k][\"x\"])) + np.random.normal(0, .1, len(result[k][\"x\"]))\n",
    "    result[k][\"y\"] = np.zeros(len(result[k][\"x\"]))\n",
    "    for i, v in enumerate(result[k][\"p\"]):\n",
    "        if v > np.median(result[k][\"p\"]):\n",
    "            result[k][\"y\"][i] = 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e5df1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.concat([pd.DataFrame(r) for r in result])\n",
    "data_df.to_csv(working_path / \"tasks.csv\", index = False)\n",
    "data_df = data_df.reset_index()\n",
    "betas_df = np.hstack([np.arange(args[\"n_tasks\"])[:, np.newaxis], np.array(zs)[:, np.newaxis], betas])\n",
    "betas_df = pd.DataFrame(betas_df)\n",
    "betas_df.columns = [\"task\", \"cluster\"] + [f\"beta{i}\" for i in range(betas.shape[1])]\n",
    "betas_df.to_csv(working_path / \"betas.csv\", index = False)\n",
    "data_dict = data_df.to_dict(orient = \"list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91363a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class nn():\n",
    "    \"\"\"\n",
    "    Neural network\n",
    "    \"\"\"\n",
    "    def __init__(self, n_inputs = 1, n_outputs = 2, H = 200):\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_inputs, H),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, H),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, H),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, H),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, n_outputs),\n",
    "        )\n",
    "    def initialize(self, n_inputs = 1, n_outputs = 2, H = 200):\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_inputs, H),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, H),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, H),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, H),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, n_outputs),\n",
    "        )\n",
    "        return self\n",
    "    def prepare_data(self, x, y):\n",
    "        if type(x) != torch.Tensor:\n",
    "            if len(x.shape) > 1:\n",
    "                x = torch.tensor(x[:, 1:]).float()\n",
    "            else:\n",
    "                x = torch.tensor(x).float()\n",
    "        if type(y) != torch.Tensor:\n",
    "            y = torch.tensor(y).float()\n",
    "        return x, y\n",
    "    def fit(self, x_train, y_train, loss_fn = torch.nn.MSELoss(), n_epochs = 10, lr = 1e-4):\n",
    "        model = self.model\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr = lr)\n",
    "        for epoch in range(n_epochs):\n",
    "            # get loss\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = self.model(x_train[:, np.newaxis])\n",
    "            loss = loss_fn(y_train, y_hat)\n",
    "\n",
    "            # update weights\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        return self\n",
    "            \n",
    "        return model\n",
    "    def evaluate(self, x_test, y_test, loss_fn = torch.nn.MSELoss()):\n",
    "        with torch.no_grad():\n",
    "            y_hat = self.model(x_test[:, np.newaxis])\n",
    "            l = loss_fn(y_test, y_hat)\n",
    "        return l\n",
    "    def pred(self, x_new):\n",
    "        with torch.no_grad():\n",
    "            y_hat = self.model(x_new)\n",
    "        return y_hat\n",
    "    def combine_with_old(self, model_old, decay_rate = .5):\n",
    "        for i in range(len(model_old)):\n",
    "            if \"weight\" in dir(model_old[i]):\n",
    "                self.model[i].weight = torch.nn.Parameter(decay_rate * model_old[i].weight + (1 - decay_rate) * self.model[i].weight)\n",
    "                self.model[i].bias = torch.nn.Parameter(decay_rate * model_old[i].bias + (1 - decay_rate) * self.model[i].bias)\n",
    "    def save(self, path = \".\", x_new = None, y_new = None, para = True):\n",
    "        x_new, y_new = self.prepare_data(x_new, y_new)\n",
    "        path = Path(path)\n",
    "        path.mkdir(parents = True, exist_ok = True)\n",
    "        y_hat = self.pred(x_new)\n",
    "        if not x_new is None:\n",
    "            pd.DataFrame.from_dict({\"x\": [item[0] for item in x_new.tolist()], \n",
    "                        \"y\": y_new,\n",
    "                        \"y_hat\": [item[0] for item in y_hat.tolist()]\n",
    "                       }).to_csv(path / Path(\"fitted.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d69e6149",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'nn' has no attribute 'CrossEntropyLoss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-1b9ffe839837>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'nn' has no attribute 'CrossEntropyLoss'"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a61614",
   "metadata": {},
   "outputs": [],
   "source": [
    "class nn():\n",
    "    \"\"\"\n",
    "    Neural network\n",
    "    \"\"\"\n",
    "    def __init__(self, n_inputs = 1, n_outputs = 2, H = 200):\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_inputs, H),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, H),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, H),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, H),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, n_outputs),\n",
    "        )\n",
    "    def initialize(self, n_inputs = 1, n_outputs = 2, H = 200):\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_inputs, H),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, H),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, H),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, H),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, n_outputs),\n",
    "        )\n",
    "        return self\n",
    "    def prepare_data(self, x, y, binary = True):\n",
    "        if type(x) != torch.Tensor:\n",
    "            if len(x.shape) > 1:\n",
    "                x = torch.tensor(x[:, 1:]).float()\n",
    "            else:\n",
    "                x = torch.tensor(x).float()\n",
    "        if (binary is True) and (len(y.shape) == 1):\n",
    "            y = np.array([y, 1-y]).T\n",
    "        if type(y) != torch.Tensor:\n",
    "            y = torch.tensor(y).float()\n",
    "        return x, y\n",
    "    def fit(self, x_train, y_train, loss_fn = torch.nn.MSELoss(), n_epochs = 10, lr = 1e-4):\n",
    "        model = self.model\n",
    "        #optimizer = torch.optim.Adam(self.model.parameters(), lr = lr)\n",
    "        optimizer = torch.optim.SGD(self.model.parameters(), lr = lr, momentum=0.9)\n",
    "        for epoch in range(n_epochs):\n",
    "            # get loss\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = self.model(x_train[:, np.newaxis])\n",
    "            loss = loss_fn(y_train, y_hat)\n",
    "\n",
    "            # update weights\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        return self\n",
    "            \n",
    "        return model\n",
    "    def evaluate(self, x_test, y_test, loss_fn = torch.nn.MSELoss()):\n",
    "        with torch.no_grad():\n",
    "            y_hat = self.model(x_test[:, np.newaxis])\n",
    "            l = loss_fn(y_test, y_hat)\n",
    "        return l\n",
    "    def pred(self, x_new):\n",
    "        with torch.no_grad():\n",
    "            y_hat = self.model(x_new)\n",
    "        return y_hat\n",
    "    def combine_with_old(self, model_old, decay_rate = .5):\n",
    "        for i in range(len(model_old)):\n",
    "            if \"weight\" in dir(model_old[i]):\n",
    "                self.model[i].weight = torch.nn.Parameter(decay_rate * model_old[i].weight + (1 - decay_rate) * self.model[i].weight)\n",
    "                self.model[i].bias = torch.nn.Parameter(decay_rate * model_old[i].bias + (1 - decay_rate) * self.model[i].bias)\n",
    "    def save(self, path = \".\", x_new = None, y_new = None, para = True, binary = True):\n",
    "        x_new, y_new = self.prepare_data(x_new, y_new)\n",
    "        path = Path(path)\n",
    "        path.mkdir(parents = True, exist_ok = True)\n",
    "        y_hat = self.pred(x_new)\n",
    "        if not x_new is None:\n",
    "            pd.DataFrame.from_dict({\"x\": [item[0] for item in x_new.tolist()], \n",
    "                        \"y\": y_new,\n",
    "                        \"y_hat\": [item[0] for item in y_hat.tolist()]\n",
    "                       }).to_csv(path / Path(\"fitted.csv\"))\n",
    "        if binary is True:\n",
    "            \n",
    "            pd.DataFrame.from_dict({\"x\": [item[0] for item in x_new.tolist()], \n",
    "                        \"y\": torch.max(y_new, 1).indices.tolist(),\n",
    "                        \"y_hat\": torch.max(y_hat, 1).indices.tolist()\n",
    "                       }).to_csv(path / Path(\"fitted.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcd2f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def subset_data(data_dict, key_name = \"task\", key_value = 0, test_size = 0.33):\n",
    "    \"\"\"\n",
    "    Subsetting data by the value of a key.\n",
    "    \n",
    "    Parameters\n",
    "    ---\n",
    "    data_dict: dict\n",
    "        the dictionary one wants to subset\n",
    "    key_name: str\n",
    "        the key one wants to subset on\n",
    "    key_value: list / int / str\n",
    "        the value of the key desirable in the output subset\n",
    "    test_size: float\n",
    "        how to split the resulting subset; if set to zero, then the output won't be splitted\n",
    "\n",
    "    Returns\n",
    "    ---\n",
    "    \n",
    "    \"\"\"\n",
    "    if type(data_dict[key_name]) == list:\n",
    "        values = data_dict[key_name]\n",
    "    else:\n",
    "        values = list(data_dict[key_name].values())\n",
    "    \n",
    "    n_task = max(values) + 1    \n",
    "    if type(key_value) != list:\n",
    "        idx_task = [i for (i, v) in enumerate(values) if v == key_value]\n",
    "    else:\n",
    "        idx_task = [i for (i, v) in enumerate(values) if v in key_value]\n",
    "        \n",
    "    tasks = [data_dict['task'][i] for i in idx_task]\n",
    "    \n",
    "    \n",
    "    x = [data_dict['x'][i] for i in idx_task]\n",
    "    y = np.array([data_dict['y'][i] for i in idx_task])\n",
    "    X = np.array([np.ones(len(idx_task)), np.array(x)]).T\n",
    "    \n",
    "    if test_size == 0:\n",
    "        return X, y, tasks\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                        test_size = test_size,\n",
    "                                                        random_state = 123,\n",
    "                                                           stratify = y)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a8bad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(data_dict, target_task, target_test_size, preprocess = True):\n",
    "    \"\"\"\n",
    "    Preparing input data for bandit selection\n",
    "    \n",
    "    Parameters\n",
    "    ---\n",
    "    data_dict: dict\n",
    "        all data, including source and target\n",
    "    target_task: int\n",
    "        data with data_dict[\"task\"] equals to target_task will be in the target\n",
    "    target_test_size: float\n",
    "        within [0, 1) indicating the proportion of the validation + test set.\n",
    "    \n",
    "    Returns\n",
    "    ---\n",
    "    input_data: dict\n",
    "        keys including data_dict, source_dict,\n",
    "                        source_task, source_cluster,\n",
    "                        X_target_train, X_target_test, X_target_val, y_target_train, y_target_test, y_target_val\n",
    "    \"\"\"\n",
    "    \n",
    "    n_tasks = max(data_dict[\"task\"]) + 1\n",
    "\n",
    "\n",
    "    input_data = {\"data_dict\": data_dict}\n",
    "    input_data[\"X_target_train\"], input_data[\"X_target_test\"], input_data[\"y_target_train\"], input_data[\"y_target_test\"] = subset_data(data_dict, key_value = target_task, key_name = \"task\",\n",
    "                                                                                                                                      test_size = target_test_size)\n",
    "    input_data[\"X_target_val\"], input_data[\"X_target_test\"], input_data[\"y_target_val\"], input_data[\"y_target_test\"] = train_test_split(input_data[\"X_target_test\"], input_data[\"y_target_test\"], \n",
    "                                                        test_size = .5,\n",
    "                                                        random_state = 123, stratify = input_data[\"y_target_test\"]  )\n",
    "        \n",
    "    input_data[\"source_task\"] = [v for v in range(n_tasks) if v != target_task]\n",
    "    \n",
    "    \n",
    "    idx_source = [i for (i, v) in enumerate(data_dict['task']) if v != target_task]\n",
    "    \n",
    "    # source data\n",
    "    input_data[\"source_dict\"] = {}\n",
    "    for key_name in data_dict.keys():\n",
    "        input_data[\"source_dict\"][key_name] = [data_dict[key_name][i] for i in idx_source]\n",
    "    \n",
    "    \n",
    "    if preprocess:\n",
    "        input_data[\"data_dict\"] = pre(raw_data = input_data[\"data_dict\"]).pre_process(key_names = [\"y\", \"x\", \"f\"], by_key = \"task\")\n",
    "        input_data[\"source_dict\"] = pre(raw_data = input_data[\"source_dict\"]).pre_process(key_names = [\"y\", \"x\"], by_key = \"task\")\n",
    "        input_data = pre(raw_data = input_data).pre_process(key_names = [\"X_target_test\", \"X_target_val\", \"X_target_train\",\n",
    "                                          \"y_target_train\", \"y_target_val\", \"y_target_test\"], by_key = None)\n",
    "    \n",
    "    return(input_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9063506e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = prepare_input(data_dict,\n",
    "                               target_task = 4,\n",
    "                               target_test_size = 0.6,\n",
    "                              preprocess = True)\n",
    "pd.DataFrame.from_dict(input_data[\"data_dict\"]).to_csv(working_path / \"tasks_processed.csv\",\n",
    "                                                           index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676c9c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.model.parameters(), lr = 1e-4, momentum=0.9)\n",
    "for epoch in range(10):\n",
    "    # get loss\n",
    "    optimizer.zero_grad()\n",
    "    y_hat = model.model(val_x[:, np.newaxis])\n",
    "    loss = loss_fn(val_y, y_hat)\n",
    "\n",
    "    # update weights\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b59f7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(3, 2, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(2)\n",
    "output = loss_fn(input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d73c7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.pred(val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab771714",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(y_hat,1).indices.type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3db8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef010d7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.nn.BCELoss()(val_y, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d06fed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(y_hat,1).indices.type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ef5f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.BCELoss()(val_y[:,0], val_y[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3177af",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "torch.nn.BCELoss()(y_hat, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6458240f",
   "metadata": {},
   "outputs": [],
   "source": [
    "?loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3697d708",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = nn()\n",
    "mod.fit(torch.Tensor(result[0][\"x\"]),\n",
    "        torch.Tensor(result[0][\"f\"]),\n",
    "        n_epochs = 100, loss_fn = loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabd6592",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = mod.pred(val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3514c157",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2381ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn(n_inputs = 1)\n",
    "n_it = 100\n",
    "batch_size = 64\n",
    "decay_rate = .5\n",
    "conservative = False\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ec47bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit_selects = [None]\n",
    "# initialize hyperparameters\n",
    "alpha = dict.fromkeys(input_data[\"source_task\"], [1])\n",
    "beta = dict.fromkeys(input_data[\"source_task\"], [1])\n",
    "pi = dict.fromkeys(input_data[\"source_task\"], [0])\n",
    "\n",
    "mod = nn()\n",
    "val_x, val_y = mod.prepare_data(input_data[\"X_target_val\"], input_data[\"y_target_val\"])\n",
    "\n",
    "# initialize model from target training data\n",
    "X_current, y_current = mod.prepare_data(input_data[\"X_target_train\"], input_data[\"y_target_train\"])\n",
    "mod.fit( X_current, y_current, n_epochs = 100, loss_fn = loss_fn)\n",
    "l = mod.evaluate(val_x, val_y, loss_fn = loss_fn)\n",
    "losses = [l]\n",
    "model_old = copy.deepcopy(mod.model)\n",
    "\n",
    "for t in range(n_it):\n",
    "    # select bandit\n",
    "    bandit_current, pi = get_bandit(input_data, alpha, beta,t, pi)\n",
    "    bandit_selects.append(bandit_current)\n",
    "    \n",
    "    # set training data at this iteration\n",
    "    X_current, y_current, _ = subset_data(input_data[\"source_dict\"], \n",
    "                               key_value = bandit_current,\n",
    "                               key_name = \"task\", test_size = 0)\n",
    "    batch_id = random.choices(list(range(0, len(y_current))), k = batch_size)\n",
    "    X_current, y_current = X_current[batch_id, :], y_current[batch_id]\n",
    "\n",
    "    #X_current = np.concatenate((X_current, input_data[\"X_target_val\"]), axis = 0)\n",
    "    #y_current = np.concatenate((y_current, input_data[\"y_target_val\"]), axis = 0)\n",
    "    \n",
    "    X_current, y_current = mod.prepare_data(X_current, y_current)\n",
    "    # train model\n",
    "    #mod = model.initialize(n_inputs = 1)\n",
    "    mod = nn()\n",
    "    mod.fit(X_current, y_current, loss_fn = loss_fn, n_epochs = 200)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # combine parameters with previous model\n",
    "    #mod.combine_with_old(model_old, decay_rate = 1)\n",
    "    \n",
    "    # evaluate model\n",
    "    l = mod.evaluate(val_x, val_y, loss_fn = loss_fn)\n",
    "    losses += [l]\n",
    "    model_old = copy.deepcopy(mod.model)\n",
    "    print(t, \", current = \", mod.model[2].weight[0,0].detach().numpy(),\n",
    "          \", old = \", model_old[2].weight[0,0].detach().numpy())\n",
    "    # update bandit parameters\n",
    "    if conservative:\n",
    "        thres = 100000\n",
    "    else:\n",
    "        thres = avg_loss(bandit_selects, losses, bandit_current)\n",
    "    alpha, beta = update_hyper_para(alpha, beta, t, losses,\n",
    "                                    bandit_current,\n",
    "                                    thres = thres\n",
    "                                   )\n",
    "    mod.save(path = working_path / (\"current\" + str(t)), x_new = X_current, y_new = y_current, para = True)\n",
    "    mod.save(path = working_path / str(t), x_new = val_x, y_new = val_y, para = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757b40c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_files(working_path, alpha, beta, losses, bandit_selects, pi, pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b14c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519a7331",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"weight\" in dir(model_old[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678288ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = nn()\n",
    "m1.initialize()\n",
    "print(m1.model[2].weight[0,0].detach().numpy(), \", old = \", m2[2].weight[0,0].detach().numpy())\n",
    "\n",
    "\n",
    "m1.fit(val_x, val_y)\n",
    "\n",
    "m2 = copy.deepcopy(m1.model)\n",
    "print(m1.model[2].weight[0,0].detach().numpy(), \", old = \", m2[2].weight[0,0].detach().numpy())\n",
    "\n",
    "\n",
    "m1.fit(X_current, y_current)\n",
    "\n",
    "print(m1.model[2].weight[0,0].detach().numpy(), \", old = \", m2[2].weight[0,0].detach().numpy())\n",
    "\n",
    "m1.combine_with_old(m2)\n",
    "print(m1.model[2].weight[0,0].detach().numpy(), \", old = \", m2[2].weight[0,0].detach().numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b6fa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = nn()\n",
    "#m1.initialize()\n",
    "m2 = copy.deepcopy(m1.model)\n",
    "print(m1.model[2].weight[0,0].detach().numpy(), \", old = \", m2[2].weight[0,0].detach().numpy())\n",
    "\n",
    "m1.fit(X_current, y_current)\n",
    "print(m1.model[2].weight[0,0].detach().numpy(), \", old = \", m2[2].weight[0,0].detach().numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d8e2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn()\n",
    "for i in range(len(model_old)):\n",
    "    if \"weight\" in dir(model_old[i]):\n",
    "        m.model[i].weight = torch.nn.Parameter(decay_rate * m2[i].weight + (1 - decay_rate) * m1.model[i].weight)\n",
    "        m.model[i].bias = torch.nn.Parameter(decay_rate * m2[i].bias + (1 - decay_rate) * m1.model[i].bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f7876f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m1.model[2].weight[0,0].detach().numpy(), \", old = \", m2[2].weight[0,0].detach().numpy(),\n",
    "     \", combined = \", m.model[2].weight[0,0].detach().numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96723613",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( m.model[2].weight[0,0].detach().numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79305d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    m.model[i].weight = torch.nn.Parameter((1 - decay_rate) * m1.model[i].weight)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c748b499",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2[0].weight\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab45ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1.model[0].weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3227653e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1.model[0].weight = m2[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f62539",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1.model[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bd959f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name, param in m.model.named_parameters():\n",
    "    param.copy_(m1.model[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd14e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.model[2].weight[0,0] = torch.nn.Parameter(torch(0))\n",
    "print(m.model[2].weight[0,0].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ea55b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1.combine_with_old(m2, decay_rate = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd78e3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(m1.model[2].weight[0,0].detach().numpy(), \", old = \", m2[2].weight[0,0].detach().numpy(),\n",
    "     \", combined = \", m1.model[2].weight[0,0].detach().numpy())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cd3fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn()\n",
    "model.fit(val_x, val_y)\n",
    "yhat = model.pred(val_x)\n",
    "pd.DataFrame.from_dict({\"x\": [item[0] for item in val_x.tolist()],\n",
    "                        \"y\": val_y.tolist(),\n",
    "                       \"yhat\": [item[0] for item in yhat.tolist()]}).to_csv(working_path / \"subset_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b766eedf",
   "metadata": {},
   "source": [
    "### Combining two nn models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2fd264",
   "metadata": {},
   "outputs": [],
   "source": [
    "class nn():\n",
    "    \"\"\"\n",
    "    Neural network\n",
    "    \"\"\"\n",
    "    def __init__(self, n_inputs = 1, n_outputs = 2, H = 200):\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_inputs, H),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, H),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, H),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, H),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, n_outputs),\n",
    "        )\n",
    "    def initialize(self, n_inputs = 1, n_outputs = 2, H = 200):\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_inputs, H),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, H),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, H),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, H),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, n_outputs),\n",
    "        )\n",
    "        return self\n",
    "    def prepare_data(self, x, y):\n",
    "        if type(x) != torch.Tensor:\n",
    "            if len(x.shape) > 1:\n",
    "                x = torch.tensor(x[:, 1:]).float()\n",
    "            else:\n",
    "                x = torch.tensor(x).float()\n",
    "        if type(y) != torch.Tensor:\n",
    "            y = torch.tensor(y).float()\n",
    "        return x, y\n",
    "    def fit(self, x_train, y_train, loss_fn = torch.nn.MSELoss(), n_epochs = 10, lr = 1e-4):\n",
    "        model = self.model\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr = lr)\n",
    "        for epoch in range(n_epochs):\n",
    "            # get loss\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = self.model(x_train[:, np.newaxis])\n",
    "            loss = loss_fn(y_train, y_hat)\n",
    "\n",
    "            # update weights\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        return self\n",
    "            \n",
    "        return model\n",
    "    def evaluate(self, x_test, y_test, loss_fn = torch.nn.MSELoss()):\n",
    "        with torch.no_grad():\n",
    "            y_hat = self.model(x_test[:, np.newaxis])\n",
    "            l = loss_fn(y_test, y_hat)\n",
    "        return l\n",
    "    def pred(self, x_new):\n",
    "        with torch.no_grad():\n",
    "            y_hat = self.model(x_new)\n",
    "        return y_hat\n",
    "    def combine_with_old(self, model_old, decay_rate = .5):\n",
    "        for i in range(len(model_old)):\n",
    "            if \"weight\" in dir(model_old[i]):\n",
    "                self.model[i].weight = torch.nn.Parameter(decay_rate * model_old[i].weight + (1 - decay_rate) * self.model[i].weight)\n",
    "                self.model[i].bias = torch.nn.Parameter(decay_rate * model_old[i].bias + (1 - decay_rate) * self.model[i].bias)\n",
    "    def save(self, path = \".\", x_new = None, y_new = None, para = True):\n",
    "        x_new, y_new = self.prepare_data(x_new, y_new)\n",
    "        path = Path(path)\n",
    "        path.mkdir(parents = True, exist_ok = True)\n",
    "        y_hat = self.pred(x_new)\n",
    "        if not x_new is None:\n",
    "            pd.DataFrame.from_dict({\"x\": [item[0] for item in x_new.tolist()], \n",
    "                        \"y\": y_new,\n",
    "                        \"y_hat\": [item[0] for item in y_hat.tolist()]\n",
    "                       }).to_csv(path / Path(\"fitted.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1c7449",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "f, betas, zs = random_functions(args[\"n_tasks\"], 6,\n",
    "                                sigma_between = sigma_setting[s][0],\n",
    "                                sigma_within = sigma_setting[s][-1])\n",
    "result = []\n",
    "for i, fi in enumerate(f):\n",
    "    x = np.random.uniform(0, 1, 100)\n",
    "    result.append({\n",
    "        \"task\": i,\n",
    "        \"x\": x,\n",
    "        \"f\": fi(x),\n",
    "        \"y\": fi(x) + np.random.normal(0, .1, len(x))\n",
    "    })\n",
    "# save data\n",
    "data_df = pd.concat([pd.DataFrame(r) for r in result])\n",
    "data_df = data_df.reset_index()\n",
    "data_dict = data_df.to_dict(orient = \"list\")\n",
    "\n",
    "input_data = prepare_input(data_dict,\n",
    "                                   target_task = 5,\n",
    "                                   target_test_size = .4,\n",
    "                                  preprocess = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd41614",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_x, val_y = mod.prepare_data(input_data[\"X_target_val\"], input_data[\"y_target_val\"])\n",
    "\n",
    "# initialize model from target training data\n",
    "X_current, y_current = mod.prepare_data(input_data[\"X_target_train\"], input_data[\"y_target_train\"])\n",
    "mod.fit( X_current, y_current, n_epochs = 100, loss_fn = loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121c5d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = nn()\n",
    "m1.fit( X_current, y_current, n_epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ed4f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1.model[2].weight[2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17eec87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m2 = nn()\n",
    "m2.fit(val_x, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc909ee6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m2.model[2].weight[2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0d247c",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1.combine_with_old(m2.model, decay_rate = .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88be7157",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1.model[2].weight[2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba653350",
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit_selects = [None]\n",
    "# initialize hyperparameters\n",
    "alpha = dict.fromkeys(input_data[\"source_task\"], [1])\n",
    "beta = dict.fromkeys(input_data[\"source_task\"], [1])\n",
    "pi = dict.fromkeys(input_data[\"source_task\"], [0])\n",
    "\n",
    "mod = nn()\n",
    "val_x, val_y = mod.prepare_data(input_data[\"X_target_val\"], input_data[\"y_target_val\"])\n",
    "\n",
    "# initialize model from target training data\n",
    "X_current, y_current = mod.prepare_data(input_data[\"X_target_train\"], input_data[\"y_target_train\"])\n",
    "mod.fit( X_current, y_current, n_epochs = 100)\n",
    "l = mod.evaluate(val_x, val_y)\n",
    "losses = [l]\n",
    "\n",
    "model_old = copy.deepcopy(mod.model)\n",
    "\n",
    "for t in range(n_it):\n",
    "    \n",
    "    # select bandit\n",
    "    bandit_current, pi = get_bandit(input_data, alpha, beta,t, pi)\n",
    "    bandit_selects.append(bandit_current)\n",
    "    \n",
    "    # set training data at this iteration\n",
    "    X_current, y_current, _ = subset_data(input_data[\"source_dict\"], \n",
    "                               key_value = bandit_current,\n",
    "                               key_name = \"task\", test_size = 0)\n",
    "    batch_id = random.choices(list(range(0, len(y_current))), k = batch_size)\n",
    "    X_current, y_current = X_current[batch_id, :], y_current[batch_id]\n",
    "    X_current = np.concatenate((X_current, input_data[\"X_target_val\"]), axis = 0)\n",
    "    y_current = np.concatenate((y_current, input_data[\"y_target_val\"]), axis = 0)\n",
    "    X_current, y_current = mod.prepare_data(X_current, y_current)\n",
    "    \n",
    "    # train model\n",
    "    #mod = model.initialize(n_inputs = 1)\n",
    "    mod = nn()\n",
    "    mod.fit(X_current, y_current, loss_fn = loss_fn, n_epochs = 200)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # combine parameters with previous model\n",
    "    #mod.combine_with_old(model_old, decay_rate = 1)\n",
    "    \n",
    "    # evaluate model\n",
    "    l = mod.evaluate(val_x, val_y, loss_fn = loss_fn)\n",
    "    losses += [l]\n",
    "    model_old = copy.deepcopy(mod.model)\n",
    "    print(t, \", current = \", mod.model[2].weight[0,0].detach().numpy(),\n",
    "          \", old = \", model_old[2].weight[0,0].detach().numpy())\n",
    "    # update bandit parameters\n",
    "    if conservative:\n",
    "        thres = 100000\n",
    "    else:\n",
    "        thres = avg_loss(bandit_selects, losses, bandit_current)\n",
    "    alpha, beta = update_hyper_para(alpha, beta, t, losses,\n",
    "                                    bandit_current,\n",
    "                                    thres = thres\n",
    "                                   )\n",
    "    mod.save(path = working_path / (\"current\" + str(t)), x_new = X_current, y_new = y_current, para = True)\n",
    "    mod.save(path = working_path / str(t), x_new = val_x, y_new = val_y, para = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40b09e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
