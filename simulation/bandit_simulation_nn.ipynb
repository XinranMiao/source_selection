{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bea0f513",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulate_module import *\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "23508a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_setting = {\"high_bw\": [10, .2],\n",
    "                \"medium_bw\": [1, .2],\n",
    "                \"low_bw\": [.5, .2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "6e61abe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"n_tasks\": 15,\n",
    "    \"conservative\": True,\n",
    "    \"target_test_size\": 0.8,\n",
    "    \"model_type\": \"nn\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e02af4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(Path('conservative_derived_data/high_bw') / \"tasks_processed.csv\")\n",
    "data_dict = data_df.to_dict(orient = \"list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "ddd42aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class nn():\n",
    "    def __init__(self, n_inputs = 1, n_outputs = 1, H = 100):\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(n_inputs, H),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, H),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, H),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, H),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(H, n_outputs),\n",
    "        )\n",
    "    def prepare_data(self, x, y):\n",
    "        if type(x) != torch.Tensor:\n",
    "            if len(x.shape) > 1:\n",
    "                x = torch.tensor(x[:, 1:]).float()\n",
    "            else:\n",
    "                x = torch.tensor(x).float()\n",
    "        if type(y) != torch.Tensor:\n",
    "            y = torch.tensor(x).float()\n",
    "        return x, y\n",
    "    def fit(self, x_train, y_train, loss_fn = torch.nn.MSELoss(), n_epochs = 10, lr = 1e-4):\n",
    "        model = self.model\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "        #losses = {\"train\": []}\n",
    "        y_hats = []\n",
    "        for epoch in range(n_epochs):\n",
    "            # get loss\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(x_train[:, np.newaxis])\n",
    "            loss = loss_fn(y_train, y_hat)\n",
    "\n",
    "            # update weights\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        return model\n",
    "    def evaluate(self, x_test, y_test, loss_fn = torch.nn.MSELoss()):\n",
    "        with torch.no_grad():\n",
    "            y_hat = self.model(x_test[:, np.newaxis])\n",
    "            l = loss_fn(y_test, y_hat)\n",
    "        return l\n",
    "    def combine_with_old(self, model_old, decay_rate = .5):\n",
    "        for i in range(len(model_old)):\n",
    "            if \"weight\" in dir(model_old[i]):\n",
    "                self.model[i].weight = torch.nn.Parameter(decay_rate * model_old[i].weight + (1 - decay_rate) * self.model[i].weight)\n",
    "                self.model[i].bias = torch.nn.Parameter(decay_rate * model_old[i].bias + (1 - decay_rate) * self.model[i].bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7836f6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miaoxinran/Documents/projects/source_selection22_after_rej/source_selection/simulation/simulate_module.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  processed = (raw - raw.min(axis = 0)) / (raw.max(axis = 0) - raw.min(axis = 0))\n"
     ]
    }
   ],
   "source": [
    "target_task = 0\n",
    "input_data = prepare_input(data_dict, target_task = target_task, target_test_size = args[\"target_test_size\"], \n",
    "                                  preprocess = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "3cdb17ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandit_source_train(input_data, model, batch_size, decay_rate, n_it, loss_fn, conservative = False):\n",
    "    bandit_selects = [None]\n",
    "    # initialize hyperparameters\n",
    "    alpha = dict.fromkeys(input_data[\"source_task\"], [1])\n",
    "    beta = dict.fromkeys(input_data[\"source_task\"], [1])\n",
    "    pi = dict.fromkeys(input_data[\"source_task\"], [0])\n",
    "    \n",
    "    mod = model\n",
    "    val_x, val_y = mod.prepare_data(input_data[\"X_target_val\"], input_data[\"y_target_val\"])\n",
    "\n",
    "    # initialize model from target training data\n",
    "    X_current, y_current = mod.prepare_data(input_data[\"X_target_train\"], input_data[\"y_target_train\"])\n",
    "    mod.fit( X_current, y_current)\n",
    "    l = mod.evaluate(val_x, val_y)\n",
    "    losses = [l]\n",
    "    model_old = mod.model\n",
    "    \n",
    "    for t in range(n_it):\n",
    "        mod = model\n",
    "        # select bandit\n",
    "        bandit_current, pi = get_bandit(input_data, alpha, beta,t, pi)\n",
    "        bandit_selects.append(bandit_current)\n",
    "        # set training data at this iteration\n",
    "        X_current, y_current, _ = subset_data(input_data[\"source_dict\"], \n",
    "                                   key_value = bandit_current,\n",
    "                                   key_name = \"task\", test_size = 0)\n",
    "        batch_id = random.choices(list(range(0, len(y_current))), k = batch_size)\n",
    "        X_current, y_current = X_current[batch_id, :], y_current[batch_id]\n",
    "\n",
    "        X_current = np.concatenate((X_current, input_data[\"X_target_val\"]), axis = 0)\n",
    "        y_current = np.concatenate((y_current, input_data[\"y_target_val\"]), axis = 0)\n",
    "\n",
    "        #------------------------------------------------\n",
    "        X_current, y_current = mod.prepare_data(X_current, y_current)\n",
    "         #------------------------------------------------\n",
    "\n",
    "\n",
    "        # train model\n",
    "\n",
    "        #---------------------------------\n",
    "        #mod_train = model.fit(X_current, y_current)\n",
    "        mod.fit(X_current, y_current)\n",
    "        #-----------------------------------\n",
    "        mod.combine_with_old(model_old, decay_rate = decay_rate)\n",
    "        #mod_pred = model # should be a combination of model and mod_pred\n",
    "        #mod_pred = pred_ensemble(input_data[\"X_target_val\"], input_data[\"X_target_val\"],\n",
    "           #                  mod_old.predict, mod_train.predict, decay_rate)\n",
    "\n",
    "        # evaluate model\n",
    "        #l = loss(input_data[\"y_target_val\"], mod_pred)\n",
    "        \n",
    "        l = mod.evaluate(val_x, val_y, loss_fn = loss_fn)\n",
    "        losses += [l]\n",
    "        model_old = mod.model\n",
    "\n",
    "\n",
    "        # update bandit parameters\n",
    "        if conservative:\n",
    "            thres = 100000\n",
    "        else:\n",
    "            thres = avg_loss(bandit_selects, losses, bandit_current)\n",
    "        alpha, beta = update_hyper_para(alpha, beta, t, losses,\n",
    "                                        bandit_current,\n",
    "                                        thres = thres\n",
    "                                       )\n",
    "    # baseline   \n",
    "    #_, prob = get_bandit(input_data, alpha, beta,t, pi)\n",
    "    #bl = baseline(input_data = input_data, pi=pi, alpha = alpha, beta = beta,\n",
    "    #              N = 10, model = model, pred_ensemble = pred_ensemble, loss = loss)\n",
    "    #bandit_weights = [prob[bd][-1] for bd in list(prob.keys())]\n",
    "    bl = None\n",
    "    bandit_weights=  None\n",
    "    return losses, alpha, beta, bandit_selects, pi, bl, bandit_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "d4dc469e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miaoxinran/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/Users/miaoxinran/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([48, 1, 1])) that is different to the input size (torch.Size([48, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "losses, alpha, beta, bandit_selects, pi, bl, bandit_weights = bandit_source_train(input_data = input_data,\n",
    "                    model = nn(), batch_size = 8, decay_rate = .5, n_it = 100, loss_fn =  torch.nn.MSELoss(),\n",
    "                    conservative = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "a4ea9f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miaoxinran/Documents/projects/source_selection22_after_rej/source_selection/simulation/simulate_module.py:78: RuntimeWarning: invalid value encountered in true_divide\n",
      "  processed = (raw - raw.min(axis = 0)) / (raw.max(axis = 0) - raw.min(axis = 0))\n",
      "/Users/miaoxinran/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/Users/miaoxinran/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([20, 1, 1])) that is different to the input size (torch.Size([20, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/miaoxinran/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([40, 1, 1])) that is different to the input size (torch.Size([40, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/miaoxinran/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([48, 1, 1])) that is different to the input size (torch.Size([48, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "for s in sigma_setting.keys():\n",
    "    # set directory\n",
    "    \n",
    "    if args[\"conservative\"]:\n",
    "        data_path = Path(args[\"model_type\"] + \"/conservative_derived_data\") \n",
    "    else:\n",
    "        data_path = Path(args[\"model_type\"] + \"/derived_data\")\n",
    "\n",
    "\n",
    "    data_path = Path(data_path)\n",
    "    working_path = data_path / s \n",
    "    working_path.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "    # generate data ------------------------------------------------\n",
    "    np.random.seed(1234)\n",
    "    f, betas, zs = random_functions(args[\"n_tasks\"], 6,\n",
    "                                    sigma_between = sigma_setting[s][0],\n",
    "                                    sigma_within = sigma_setting[s][-1])\n",
    "    result = []\n",
    "\n",
    "    for i, fi in enumerate(f):\n",
    "        x = np.random.uniform(0, 1, 100)\n",
    "        result.append({\n",
    "            \"task\": i,\n",
    "            \"x\": x,\n",
    "            \"f\": fi(x),\n",
    "            \"y\": fi(x) + np.random.normal(0, .1, len(x))\n",
    "        })\n",
    "\n",
    "    # save data\n",
    "    data_df = pd.concat([pd.DataFrame(r) for r in result])\n",
    "    data_df.to_csv(working_path / \"tasks.csv\", index = False)\n",
    "    data_df = data_df.reset_index()\n",
    "\n",
    "\n",
    "    betas_df = np.hstack([np.arange(args[\"n_tasks\"])[:, np.newaxis], np.array(zs)[:, np.newaxis], betas])\n",
    "    betas_df = pd.DataFrame(betas_df)\n",
    "    betas_df.columns = [\"task\", \"cluster\"] + [f\"beta{i}\" for i in range(betas.shape[1])]\n",
    "    betas_df.to_csv(working_path / \"betas.csv\", index = False)\n",
    "\n",
    "    # relationship between tasks (bandits) and their original clusters\n",
    "    #d = dict.fromkeys(betas_df.cluster, [])\n",
    "    #for k, v in zip(betas_df.cluster, betas_df.task):\n",
    "    #    d[k] = d[k] +[v]\n",
    "\n",
    "    data_dict = data_df.to_dict(orient = \"list\")\n",
    "\n",
    "    # add key \"cluster\" to `data_dict`\n",
    "    #data_dict[\"cluster\"] = []\n",
    "    #for task in data_dict[\"task\"]:\n",
    "        #cluster = get_key(d, task)\n",
    "        #if(cluster == \"There is no such key\"):\n",
    "            #print(\"task = \" + str(task))\n",
    "            #break\n",
    "        #data_dict[\"cluster\"].append(cluster)\n",
    "\n",
    "\n",
    "    # bandit selection ------------------------------------------------\n",
    "    for target_task in range(args[\"n_tasks\"]):\n",
    "        input_data = prepare_input(data_dict, target_task = target_task, target_test_size = args[\"target_test_size\"], \n",
    "                                  preprocess = True)\n",
    "        pd.DataFrame.from_dict(input_data[\"data_dict\"]).to_csv(working_path / \"tasks_processed.csv\", index = False)\n",
    "\n",
    "        losses, alpha, beta, bandit_selects, pi, bl, bandit_weights = bandit_source_train(input_data = input_data,\n",
    "                    model = nn(), batch_size = 8, decay_rate = .5, n_it = 100, loss_fn =  torch.nn.MSELoss(),\n",
    "                    conservative = args[\"conservative\"])\n",
    "\n",
    "        output_dir = working_path / f\"target_{target_task}_{args['target_test_size']}\"\n",
    "        save_files(output_dir, alpha, beta, losses, bandit_selects, pi, bl, bandit_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "c7d542ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_files(output_dir, alpha, beta, losses, bandit_selects, pi, bl, bandit_weights):\n",
    "    output_dir.mkdir(exist_ok = True)\n",
    "    pd.DataFrame.from_dict(alpha).to_csv(output_dir / \"alpha.csv\")\n",
    "    pd.DataFrame.from_dict(beta).to_csv(output_dir / \"beta.csv\")\n",
    "    pd.DataFrame.from_dict({\"losses\": [l.item() for l in losses], \"bandit_selects\": bandit_selects}).to_csv(output_dir / \"losses.csv\")\n",
    "    pd.DataFrame.from_dict(pi).to_csv(output_dir / \"pi.csv\")\n",
    "    pd.DataFrame.from_dict(bl).to_csv(output_dir / \"baseline.csv\")\n",
    "    pd.DataFrame.from_dict(bandit_weights).to_csv(output_dir / \"bandit_weights.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "cf624105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_weighted_samples(input_data, alpha, beta):  \n",
    "    weight_dict = {t: alpha[t][-1] / (alpha[t][-1] + beta[t][-1]) for t in input_data[\"source_task\"]}\n",
    "    s = sum(weight_dict.values())\n",
    "    weight_dict = {t: weight_dict[t] / s for t in weight_dict.keys()}\n",
    "\n",
    "\n",
    "    n_tasks = len(weight_dict.keys())\n",
    "    draw = np.random.multinomial(n = 1, pvals = list(weight_dict.values()), size = 100 )\n",
    "    col_idx = 0\n",
    "    X_end, y_end = None, None\n",
    "    for col_idx in range(n_tasks):\n",
    "        X_task, y_task, _ = subset_data(input_data[\"source_dict\"],\n",
    "                                        key_value = list(weight_dict.keys())[col_idx],\n",
    "                                        key_name = \"task\", test_size = 0)\n",
    "\n",
    "        non_zero_idx = np.array([i for (i, v) in enumerate(draw[:, col_idx]) if v == 1])\n",
    "\n",
    "        X_task = X_task[non_zero_idx, :]\n",
    "        y_task = y_task[non_zero_idx]\n",
    "\n",
    "        if X_end is None:\n",
    "            X_end, y_end = X_task, y_task\n",
    "        else:\n",
    "            X_end = np.concatenate((X_end, X_task), axis = 0)\n",
    "            y_end = np.concatenate((y_end, y_task), axis = 0)\n",
    "        col_idx += 1\n",
    "        \n",
    "    X_end = np.concatenate((X_end, input_data[\"X_target_val\"]), axis = 0)\n",
    "    y_end = np.concatenate((y_end, input_data[\"y_target_val\"]), axis = 0)\n",
    "    \n",
    "    return X_end, y_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "6d17dc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline(input_data, alpha, beta, model,  loss_fn, N):\n",
    "    final_loss = dict.fromkeys([\"bandit\", \"all_source\", \"target_train\", \"random_source\"], [])\n",
    "    \n",
    "    # weighted all source, by bandit selection parameters ----\n",
    "    mod = model\n",
    "    X_end, y_end = draw_weighted_samples(input_data, alpha, beta)\n",
    "    X_end, y_end = mod.prepare_data(X_end, y_end)\n",
    "\n",
    "    mod.fit(X_end, y_end)\n",
    "    test_x, test_y = mod.prepare_data(input_data[\"X_target_test\"], input_data[\"y_target_test\"])\n",
    "    final_loss[\"bandit\"] = [mod.evaluate(test_x, test_y, loss_fn = loss_fn).item()]\n",
    "    \n",
    "    # All sources----\n",
    "    mod = model\n",
    "    X_sources, y_sources = mod.prepare_data(input_data[\"source_dict\"][\"x\"], input_data[\"source_dict\"][\"y\"])\n",
    "    mod.fit(X_sources, y_sources)\n",
    "    final_loss[\"all_source\"] = [mod.evaluate(test_x, test_y, loss_fn = loss_fn).item()]\n",
    "    \n",
    "    # target train ---\n",
    "    mod = model\n",
    "    X_train, y_train = mod.prepare_data(input_data[\"X_target_train\"], input_data[\"y_target_train\"])\n",
    "    mod_train = model.fit(X_train, y_train)\n",
    "   \n",
    "    final_loss[\"target_train\"] =[ mod.evaluate(test_x, test_y, loss_fn = loss_fn).item()]\n",
    "\n",
    "    # One random source + target train ----\n",
    "    mod = model\n",
    "    for n in range(N):\n",
    "        # one random source\n",
    "        X_random, y_random, _ = subset_data(input_data[\"data_dict\"],\n",
    "                                            key_value = random.choice(input_data[\"source_task\"]),\n",
    "                                            key_name = \"task\", test_size = 0)\n",
    "        X_random = np.concatenate((X_random, input_data[\"X_target_train\"]), axis = 0)\n",
    "        y_random = np.concatenate((y_random, input_data[\"y_target_train\"]), axis = 0)\n",
    "        X_random, y_random = mod.prepare_data(X_random, y_random)\n",
    "\n",
    "        mod.fit(X_random, y_random)\n",
    "        final_loss[\"random_source\"] =[ mod.evaluate(test_x, test_y, loss_fn = loss_fn)]\n",
    "    final_loss[\"random_source\"] = [np.mean(final_loss[\"random_source\"])]\n",
    "    \n",
    "    return(final_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "3eb9f42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miaoxinran/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bandit': [0.20728923380374908],\n",
       " 'all_source': [0.17705605924129486],\n",
       " 'target_train': [0.15036506950855255],\n",
       " 'random_source': [0.083138764]}"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline(input_data, alpha, beta, model = nn(),  loss_fn = torch.nn.MSELoss(), N = 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
