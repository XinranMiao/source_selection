{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a296089",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import *\n",
    "from train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ca6210b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import collections\n",
    "\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2e367f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "EuroSat_Type = 'ALL'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8768cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = str(sys.argv)\n",
    "target_task = args[1]\n",
    "algorithm = args[2]\n",
    "algorithm = \"bandit\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05d15f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_task = \"France\"#\"Lietuva\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27645bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "output_path = Path(\"derived_data\")\n",
    "output_path.mkdir(parents = True, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f9043dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EuroSat_Type == 'RGB':\n",
    "  data_folder = '/content/sample_data/'\n",
    "  #root = os.path.join(data_folder, '2750/')\n",
    "  root = '2750/'\n",
    "  download_ON = os.path.exists(root)\n",
    "\n",
    "  if not download_ON:\n",
    "    # This can be long...\n",
    "    #os.chdir(data_folder)\n",
    "    os.system('wget http://madm.dfki.de/files/sentinel/EuroSAT.zip') #Just RGB Bands\n",
    "    !unzip EuroSAT.zip\n",
    "    download_ON = True\n",
    "elif EuroSat_Type == 'ALL':\n",
    "    root = 'ds/images/remote_sensing/otherDatasets/sentinel_2/tif/'\n",
    "    download_ON = os.path.exists(root)\n",
    "    if not download_ON:\n",
    "      os.system('wget http://madm.dfki.de/files/sentinel/EuroSATallBands.zip') #All bands\n",
    "      !unzip EuroSATallBands.zip\n",
    "      download_ON = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98a5f1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df = pd.read_csv(\"metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55ad5e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torchvision.datasets.DatasetFolder(root=root,loader = iloader, transform=None, extensions = 'tif')\n",
    "labels = [v[1] for (i, v) in enumerate(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2e0fc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_labels(input_data, labels):\n",
    "    \"\"\"\n",
    "    check labels across source / target train / target validation / target test sets, and keep labels in common\n",
    "    \"\"\"\n",
    "    train_labels = [labels[i] for i in input_data[\"idx_train\"]]\n",
    "    val_labels = [labels[i] for i in input_data[\"idx_val\"]]\n",
    "    test_labels = [labels[i] for i in input_data[\"idx_test\"]]\n",
    "    source_labels = [labels[i] for i in input_data[\"idx_source\"]]\n",
    "\n",
    "    common_labels = list(set(train_labels).intersection(val_labels).intersection(test_labels).intersection(source_labels))\n",
    "\n",
    "    input_data[\"idx_train\"] = [i for i in input_data[\"idx_train\"] if labels[i] in common_labels]\n",
    "    input_data[\"idx_test\"] = [i for i in input_data[\"idx_test\"] if labels[i] in common_labels]\n",
    "    input_data[\"idx_val\"] = [i for i in input_data[\"idx_val\"] if labels[i] in common_labels]\n",
    "    input_data[\"idx_source\"] = [i for i in input_data[\"idx_source\"] if labels[i] in common_labels]\n",
    "    \n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8aead43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input_data(geo_df, target_task = \"France\", labels = None, train_size = 640, val_size = 160, test_size = 160):\n",
    "    geo_dict = geo_df.to_dict()\n",
    "    countries = list(set(geo_dict[\"country\"].values()))\n",
    "    countries = [x for x in countries if str(x) != \"nan\"]\n",
    "    id_countries = dict.fromkeys(countries)\n",
    "    for k in id_countries.keys():\n",
    "        id_countries[k] = [v for (i, v) in enumerate(geo_dict[\"id\"]) if geo_dict[\"country\"][i] == k]\n",
    "\n",
    "    \n",
    "    input_data = {\n",
    "        \"source_task\": list(set(id_countries.keys()) - set(target_task)),\n",
    "        \"target_task\": target_task\n",
    "    }\n",
    "    \n",
    "    # define a dictionary for all data\n",
    "    \n",
    "    input_data[\"data_dict\"] = {}\n",
    "    for k in geo_dict.keys():\n",
    "        input_data[\"data_dict\"][k] = [geo_dict[k][i] for (i, v) in enumerate(geo_dict[\"country\"].values()) if str(v) != \"nan\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "    # split indices to source and target\n",
    "    \n",
    "    input_data[\"idx_source\"] = [i for (i, v) in enumerate(input_data[\"data_dict\"]['country']) if v != input_data[\"target_task\"]]\n",
    "    input_data[\"idx_target\"] = [i for (i, v) in enumerate(input_data[\"data_dict\"]['country']) if v == input_data[\"target_task\"]]\n",
    "    \n",
    "    \n",
    "    # define a dictionary for source data\n",
    "    \n",
    "    input_data[\"source_dict\"] = {}\n",
    "    for k in geo_dict.keys():\n",
    "        input_data[\"source_dict\"][k] = [input_data[\"data_dict\"][k][i] for i in input_data[\"idx_source\"]]\n",
    "\n",
    "    \n",
    "    if labels is None:\n",
    "        input_data[\"idx_train\"] = random.sample(input_data[\"idx_target\"], train_size)\n",
    "        idx_rest = list(set(input_data[\"idx_target\"]) - set(input_data[\"idx_train\"]))\n",
    "        input_data[\"idx_test\"] = random.sample(idx_rest, test_size)\n",
    "        input_data[\"idx_val\"] = list(set(idx_rest) - set(input_data[\"idx_test\"]))\n",
    "        input_data[\"idx_val\"] = random.sample(input_data[\"idx_test\"], val_size)\n",
    "    else:\n",
    "        target_labels = [labels[i] for i in input_data[\"idx_target\"]]\n",
    "        input_data[\"idx_train\"], idx_rest, _, _ = train_test_split(input_data[\"idx_target\"], target_labels, test_size = .3, random_state = 0,\n",
    "                                 shuffle = True)\n",
    "        idx_rest = list(set(input_data[\"idx_target\"]) - set(input_data[\"idx_train\"]))\n",
    "        input_data[\"idx_val\"], input_data[\"idx_test\"], _, _ = train_test_split(idx_rest, \n",
    "                                                                               [labels[i] for i in idx_rest],\n",
    "                                                                               test_size = .5, random_state = 0, shuffle = True)\n",
    "        input_data = check_labels(input_data, labels)\n",
    "    return input_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "528edc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = prepare_input_data(geo_df, target_task, labels = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c51c7af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db5ad7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data ---\n",
    "\n",
    "target_val_loader =  torch.utils.data.DataLoader(torch.utils.data.Subset(data, input_data[\"idx_val\"]), \n",
    "                                              batch_size = 16, shuffle = True, num_workers = 0)\n",
    "target_train_loader =  torch.utils.data.DataLoader(torch.utils.data.Subset(data, input_data[\"idx_train\"]), \n",
    "                                                  batch_size = 16, shuffle = True, num_workers = 0)\n",
    "target_test_loader =  torch.utils.data.DataLoader(torch.utils.data.Subset(data, input_data[\"idx_test\"]), \n",
    "                                                  batch_size = 16, shuffle = True, num_workers = 0)\n",
    "\n",
    "\n",
    "\n",
    "# initialize hyperparameters ---\n",
    "\n",
    "bandit_selects = [None]\n",
    "alpha = dict.fromkeys(input_data[\"source_task\"], [1])\n",
    "beta = dict.fromkeys(input_data[\"source_task\"], [1])\n",
    "pi = dict.fromkeys(input_data[\"source_task\"], [0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eef1bed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandit_selection(data, input_data, n_epochs = 3, n_it = 2, algorithm = \"bandit\",iter_samples = 160,\n",
    "                     lr = .01, milestones = milestones,\n",
    "                     criteria = criteria, output_path = \".\"):\n",
    "    # prepare data ---\n",
    "    \n",
    "    target_val_loader =  torch.utils.data.DataLoader(torch.utils.data.Subset(data, input_data[\"idx_val\"]), \n",
    "                                                  batch_size = 16, shuffle = True, num_workers = 0)\n",
    "    target_train_loader =  torch.utils.data.DataLoader(torch.utils.data.Subset(data, input_data[\"idx_train\"]), \n",
    "                                                      batch_size = 16, shuffle = True, num_workers = 0)\n",
    "    target_test_loader =  torch.utils.data.DataLoader(torch.utils.data.Subset(data, input_data[\"idx_test\"]), \n",
    "                                                      batch_size = 16, shuffle = True, num_workers = 0)\n",
    "    \n",
    "\n",
    "    \n",
    "    # initialize hyperparameters ---\n",
    "    train_log = []\n",
    "    bandit_selects = [None]\n",
    "    alpha = dict.fromkeys(input_data[\"source_task\"], [1])\n",
    "    beta = dict.fromkeys(input_data[\"source_task\"], [1])\n",
    "    pi = dict.fromkeys(input_data[\"source_task\"], [0])\n",
    "    \n",
    "    \n",
    "    # initialize model ---\n",
    "   \n",
    "    net = Load_model()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones = milestones, gamma=0.1)\n",
    "    if torch.cuda.is_available():\n",
    "        net=net.cuda()\n",
    "\n",
    "    net, val_acc, _, _ = train(net, target_train_loader, target_test_loader , criteria, optimizer, 2, scheduler)\n",
    "\n",
    "    print(\"Model initiated with acc \", val_acc[-1])\n",
    "    accs = [val_acc[-1]]\n",
    "    \n",
    "    # train ---\n",
    "    \n",
    "    for t in range(n_it):\n",
    "        if algorithm == \"bandit\":\n",
    "            bandit_current, pi = get_bandit(input_data, alpha, beta,t, pi)\n",
    "            bandit_selects.append(bandit_current)\n",
    "            print(\"---\", \"At iteration \", t, \", source country is \", bandit_current, \"-----\\n\")\n",
    "            current_id = [input_data[\"source_dict\"][\"id\"][i] for (i, v) in enumerate(input_data[\"source_dict\"]['country']) if v == bandit_current]\n",
    "            current_id = random.choices(current_id, k = iter_samples)\n",
    "        else:\n",
    "            bandit_current = 0\n",
    "            current_id = random.sample(input_data[\"idx_source\"], k = iter_samples)\n",
    "        current_loader = torch.utils.data.DataLoader(torch.utils.data.Subset(data, input_data[\"idx_test\"]), \n",
    "                                                          batch_size = 16, shuffle = True, num_workers = 0)\n",
    "        net, val_acc, train_acc, train_losses = train(net, current_loader, target_test_loader , criteria, optimizer, 2, scheduler)\n",
    "        \n",
    "        print(\"At iteration \", t, \", source country is \", bandit_current, \", acc is \", val_acc[-1])\n",
    "        accs += [val_acc[-1]]\n",
    "        \n",
    "        \n",
    "        # save logs\n",
    "        train_log.append({\"iter\": [t for i in range(n_it)],\n",
    "                          \"train_acc\": val_acc.tolist(),\n",
    "                          \"val_acc\": val_acc.tolist(),\n",
    "                          \"train_losses\": train_losses.tolist()})\n",
    "        \n",
    "        if algorithm == \"bandit\":\n",
    "            alpha, beta = update_hyper_para(alpha, beta, t, accs,\n",
    "                                            bandit_current\n",
    "                                           )\n",
    "        if not output_path is None:\n",
    "            if t % 10 == 0:\n",
    "                torch.save(net.state_dict(), output_path / Path(input_data[\"target_task\"] + \"_\" + algorithm + \".pt\" ))\n",
    "                save_output(output_path / Path(input_data[\"target_task\"] + \"_\" + algorithm + \"_evaluation.csv\" ), accs, accs)\n",
    "                \n",
    "                print(train_log)\n",
    "                log_df = pd.concat([pd.DataFrame(r) for r in train_log])\n",
    "                log_df.to_csv(output_path /  Path(input_data[\"target_task\"] + \"_\" + algorithm + \"train_log.csv\"))\n",
    "\n",
    "                if algorithm == \"bandit\":\n",
    "                    pd.DataFrame.from_dict(alpha).to_csv(output_path /  Path(input_data[\"target_task\"] + \"_\" + algorithm + \"alpha.csv\"))\n",
    "                    pd.DataFrame.from_dict(beta).to_csv(output_path /  Path(input_data[\"target_task\"] + \"_\" + algorithm + \"beta.csv\"))\n",
    "                    pd.DataFrame.from_dict(pi).to_csv(output_path / Path(input_data[\"target_task\"] + \"_\" + algorithm +  \"pi.csv\"))\n",
    "    return net, bandit_selects, accs, alpha, beta, pi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc04a539",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 16 and the array at index 1 has size 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-f22af02aa571>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                                                             \u001b[0mn_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_it\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                                             \u001b[0malgorithm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m160\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                                                            output_path = output_path)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-c3dab87989c7>\u001b[0m in \u001b[0;36mbandit_selection\u001b[0;34m(data, input_data, n_epochs, n_it, algorithm, iter_samples, lr, milestones, criteria, output_path)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mnet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_test_loader\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mcriteria\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model initiated with acc \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/source_selection/experiment_eurosat/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, train_, val_, criterion, optimizer, epochs, scheduler, weights, save_epoch, plot)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;31m# make predictions on the validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/source_selection/experiment_eurosat/train.py\u001b[0m in \u001b[0;36mvalidation\u001b[0;34m(model, test_)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mall_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                 \u001b[0mall_gt\u001b[0m  \u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_gt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mtest_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_iter\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0marrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 16 and the array at index 1 has size 4"
     ]
    }
   ],
   "source": [
    "_, bandit_selects, accs, alpha, beta, pi = bandit_selection(data, input_data, \n",
    "                                                            n_epochs = 1, n_it = 2,\n",
    "                                                            algorithm = algorithm, iter_samples = 160,\n",
    "                                                           output_path = output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5397fa0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
